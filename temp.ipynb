{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "# 모델과 토크나이저 초기화\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'have'), (1, 'you'), (2, 'ever'), (3, 'read'), (4, '/'), (5, 'watched'), (6, 'game'), (7, 'of'), (8, 'throne'), (9, '##s'), (10, '?'), (11, 'in'), (12, 'that'), (13, 'show'), (14, 'house'), (15, 'lan'), (16, '##nist'), (17, '##er'), (18, 'has'), (19, 'crimson'), (20, 'red'), (21, 'as'), (22, 'their'), (23, 'family'), (24, 'color'), (25, '.'), (26, '[SEP]'), (27, 'i'), (28, 'have'), (29, 'heard'), (30, 'so'), (31, 'much'), (32, 'about'), (33, 'it'), (34, ','), (35, 'i'), (36, 'have'), (37, 'never'), (38, 'watched'), (39, 'it'), (40, 'before'), (41, '!'), (42, 'it'), (43, 'looks'), (44, 'like'), (45, 'the'), (46, 'series'), (47, 'is'), (48, 'ending'), (49, 'next'), (50, 'year'), (51, 'after'), (52, 'eight'), (53, 'seasons'), (54, '!'), (55, '!')]\n"
     ]
    }
   ],
   "source": [
    "# 예제 문장과 토큰 위치\n",
    "sentence =  \"Have you ever read/watched Game of Thrones? In that show House Lannister has crimson red as their family color. [SEP] I have heard so much about It, I have never watched it before! It looks like the series is ending next year after eight seasons!!\"\n",
    "# token_index = 6  # [MASK]로 대체할 토큰의 위치\n",
    "candidates = ['\"crimson. I prefer baby pink\"', 'red', 'Game of Thrones', 'House Lannister']  # 후보 단어 리스트\n",
    "\n",
    "# 문장을 토큰화하고 [MASK] 토큰으로 지정된 위치의 단어 대체\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "#print index and token in tokens\n",
    "print([(i,t) for i,t in enumerate(tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'have',\n",
       " 'you',\n",
       " 'ever',\n",
       " 'read',\n",
       " '/',\n",
       " 'watched',\n",
       " 'game',\n",
       " 'of',\n",
       " 'throne',\n",
       " '##s',\n",
       " '?',\n",
       " 'in',\n",
       " 'that',\n",
       " 'show',\n",
       " 'house',\n",
       " 'lan',\n",
       " '##nist',\n",
       " '##er',\n",
       " 'has',\n",
       " 'crimson',\n",
       " 'red',\n",
       " 'as',\n",
       " 'their',\n",
       " 'family',\n",
       " 'color',\n",
       " '.',\n",
       " '[SEP]',\n",
       " 'i',\n",
       " 'have',\n",
       " 'heard',\n",
       " 'so',\n",
       " 'much',\n",
       " 'about',\n",
       " '[MASK]',\n",
       " ',',\n",
       " 'i',\n",
       " 'have',\n",
       " 'never',\n",
       " 'watched',\n",
       " 'it',\n",
       " 'before',\n",
       " '!',\n",
       " 'it',\n",
       " 'looks',\n",
       " 'like',\n",
       " 'the',\n",
       " 'series',\n",
       " 'is',\n",
       " 'ending',\n",
       " 'next',\n",
       " 'year',\n",
       " 'after',\n",
       " 'eight',\n",
       " 'seasons',\n",
       " '!',\n",
       " '!',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_index = 33\n",
    "tokens[token_index] = '[MASK]'\n",
    "mask_token_index = tokens.index('[MASK]')\n",
    "\n",
    "# [CLS]와 [SEP] 토큰 추가\n",
    "tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "mask_token_index += 1  # [CLS] 토큰이 추가되어 인덱스 조정\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('red', 0.0003169028495904058),\n",
       " ('\"crimson. I prefer baby pink\"', 8.241360660576902e-08),\n",
       " ('Game of Thrones', 8.241360660576902e-08),\n",
       " ('House Lannister', 8.241360660576902e-08)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예제 문장과 토큰 위치\n",
    "sentence =  \"Have you ever read/watched Game of Thrones? In that show House Lannister has crimson red as their family color. [SEP] I have heard so much about It, I have never watched it before! It looks like the series is ending next year after eight seasons!!\"\n",
    "# token_index = 6  # [MASK]로 대체할 토큰의 위치\n",
    "candidates = ['\"crimson. I prefer baby pink\"', 'red', 'Game of Thrones', 'House Lannister']  # 후보 단어 리스트\n",
    "\n",
    "# 문장을 토큰화하고 [MASK] 토큰으로 지정된 위치의 단어 대체\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "#print index and token in tokens\n",
    "# print([(i,t) for i,t in enumerate(tokens)])\n",
    "\n",
    "token_index = 33\n",
    "tokens[token_index] = '[MASK]'\n",
    "mask_token_index = tokens.index('[MASK]')\n",
    "\n",
    "# [CLS]와 [SEP] 토큰 추가\n",
    "tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "mask_token_index += 1  # [CLS] 토큰이 추가되어 인덱스 조정\n",
    "\n",
    "# 토큰을 모델의 입력 형태로 변환\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_tensor = torch.tensor([input_ids])\n",
    "\n",
    "# [MASK] 위치에 대한 예측 수행\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_tensor)\n",
    "    predictions = outputs[0]\n",
    "\n",
    "# 후보 단어들의 확률 계산\n",
    "candidate_ids = tokenizer.convert_tokens_to_ids(candidates)\n",
    "probs = torch.softmax(predictions[0, mask_token_index], dim=-1)\n",
    "candidate_probs = probs[candidate_ids]\n",
    "\n",
    "# 후보 단어들과 그 확률 출력\n",
    "results = {}\n",
    "for word, prob in zip(candidates, candidate_probs):\n",
    "    # print(f\"{word}: {prob.item()}\")\n",
    "    results[word] = prob.item()\n",
    "\n",
    "# 확률이 높은 순서대로 출력\n",
    "sorted(results.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils_select_predictions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "import json, jsonlines\n",
    "import logging\n",
    "from collections import Counter\n",
    "import argparse\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 모델과 토크나이저 초기화\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "\n",
    "def write_json(data, file_path):\n",
    "    \"\"\"\n",
    "    Write data to a JSON file. w/ args.output_file\n",
    "    \"\"\"\n",
    "    logging.info(f'Writing {len(data)} items to {file_path}.')\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    \n",
    "def evaluate_sentence_with_candidates(sentence_template, candidates):\n",
    "    # 모델과 토크나이저 초기화\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    model.eval()\n",
    "\n",
    "    # 로그 확률을 계산하는 함수\n",
    "    def score(sentence):\n",
    "        tokenize_input = tokenizer.encode(sentence)\n",
    "        tensor_input = torch.tensor([tokenize_input])\n",
    "        with torch.no_grad():\n",
    "            loss = model(tensor_input, labels=tensor_input)[0]\n",
    "        return -loss.item()\n",
    "\n",
    "    # 각 후보에 대한 문장의 확률 계산\n",
    "    scores = {}\n",
    "    for candidate in candidates:\n",
    "        candidate_sentence = sentence_template.format(candidate)\n",
    "        candidate_score = score(candidate_sentence)\n",
    "        scores[candidate] = candidate_score\n",
    "\n",
    "    # 확률이 높은 순서대로 후보 정렬 및 출력\n",
    "    sorted_candidates = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return sorted_candidates\n",
    "\n",
    "# def generate_filled_sentences(found_pronoun, context, response, pronoun_index, candidates):\n",
    "#     # 문맥에서 마지막 두 문장을 가져옵니다.\n",
    "#     context_part = \" \".join(context[-2:])\n",
    "    \n",
    "#     # 대응하는 응답에서 대명사를 {}로 대체합니다.\n",
    "#     doc = nlp(response)\n",
    "#     if doc[pronoun_index] == found_pronoun: # 지시대명사를 빈칸으로 대체하기\n",
    "#         doc[pronoun_index] = '{}'\n",
    "#     response_with_placeholder = \" \".join(doc)\n",
    "    \n",
    "#     # 완성된 문장 템플릿을 생성합니다.\n",
    "#     sentence_template = f\"{context_part} {response_with_placeholder}\"\n",
    "    \n",
    "#     # 각 후보에 대한 문장의 확률을 계산하고 정렬합니다.\n",
    "#     sorted_candidates = evaluate_sentence_with_candidates(sentence_template, candidates)\n",
    "    \n",
    "#     return sorted_candidates\n",
    "\n",
    "def generate_filled_sentences(found_pronoun, context, response, pronoun_index, candidates):\n",
    "    # 문맥에서 마지막 두 문장을 가져옵니다.\n",
    "    context_part = \" \".join(context[-2:])\n",
    "    \n",
    "    # 대응하는 응답을 토큰화합니다.\n",
    "    doc = nlp(response)\n",
    "    tokens = [token.text for token in doc]  # spacy 토큰을 문자열 리스트로 변환\n",
    "    \n",
    "    # 지시대명사의 인덱스에 해당하는 토큰을 '{}'로 대체합니다.\n",
    "    if tokens[pronoun_index] == found_pronoun:\n",
    "        tokens[pronoun_index] = '{}'\n",
    "    response_with_placeholder = \" \".join(tokens)  # 수정된 토큰 리스트를 다시 문자열로 결합\n",
    "    \n",
    "    # 완성된 문장 템플릿을 생성합니다.\n",
    "    sentence_template = f\"{context_part} {response_with_placeholder}\"\n",
    "    \n",
    "    # 각 후보에 대한 문장의 확률을 계산하고 정렬합니다.\n",
    "    sorted_candidates = evaluate_sentence_with_candidates(sentence_template, candidates)\n",
    "    \n",
    "    return sorted_candidates\n",
    "\n",
    "def select_predictions(frame_file, pred_file, output_file):\n",
    "    # logging.info('>>>>>>>>>>>>>>>>> Start selecting predictions.')\n",
    "    print('>>>>>>>>>>>>>>>>> Start selecting predictions.')\n",
    "    frames = []\n",
    "    predictions = []\n",
    "    with jsonlines.open(frame_file) as reader:\n",
    "        for line in reader:\n",
    "            frames.append(line)\n",
    "    with open(pred_file, 'r') as f:\n",
    "        predictions = json.load(f)\n",
    "\n",
    "    # assert len(frames) == len(predictions)\n",
    "    # logging.info('>>>>>>>>>>>>>>>>> len(frames): {}'.format(len(frames)))\n",
    "    # logging.info('>>>>>>>>>>>>>>>>> len(preds): {}'.format(len(predictions)))\n",
    "    print('>>>>>>>>>>>>>>>>> len(frames): {}'.format(len(frames)))\n",
    "    print('>>>>>>>>>>>>>>>>> len(preds): {}'.format(len(predictions)))\n",
    "    return\n",
    "    results = {}\n",
    "\n",
    "    qasid_empty = []\n",
    "    for i, frame in enumerate(frames):\n",
    "        candidates = [cand[\"text\"] for cand in predictions[str(i)]]\n",
    "        sorted_candidates = generate_filled_sentences(\n",
    "                                                        frame['found_pronoun'],\n",
    "                                                        frame[\"context_text\"],\n",
    "                                                        frame[\"orig_response\"],\n",
    "                                                        frame[\"pronoun_index\"],\n",
    "                                                        candidates\n",
    "                                                    )   \n",
    "        \n",
    "        if not sorted_candidates: # No predictions: \"empty\" in predictsion_.json, [] in nbest_predictions_.json\n",
    "            logging.info(f'[Empty] ---> {frame[\"qas_id\"]}')\n",
    "            qasid_empty.append(frame['qas_id'])\n",
    "        else:\n",
    "            predicted_noun = sorted_candidates[0][0]\n",
    "            logging.info(f'[{predicted_noun}] ---> {frame[\"qas_id\"]}')\n",
    "            results[frame['qas_id']] = [predicted_noun]\n",
    "        # break\n",
    "        # print logging every 10% in for loop\n",
    "        if i % (len(frames) // 10) == 0:\n",
    "            logging.info(f'>>>>>>>>>>>>>>>>> {i} / {len(frames)} done.')\n",
    "    write_json(results, output_file)\n",
    "    print(f'>>>>>>>>>>>>>>>>> The Number of empty predictions: {len(qasid_empty)} out of {len(frames)}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dialfact / Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>> Start selecting predictions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>> len(frames): 7411\n",
      ">>>>>>>>>>>>>>>>> len(preds): 7411\n"
     ]
    }
   ],
   "source": [
    "# [1] dialfact valid\n",
    "FRAME_FILE='/data/scratch/acw722/corefbert/result/resolved_data/dialfact/dialfact_valid_with_pronouns.jsonl'\n",
    "PRED_FILE='/data/scratch/acw722/corefbert/result/inference/nbest_predictions_dialfact_valid.json'\n",
    "OUTPUT_FILE='/data/scratch/acw722/corefbert/result/inference/thebest_predictions_dialfact_valid.json'\n",
    "\n",
    "select_predictions(FRAME_FILE, PRED_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>> Start selecting predictions.\n",
      ">>>>>>>>>>>>>>>>> len(frames): 10155\n",
      ">>>>>>>>>>>>>>>>> len(preds): 10155\n"
     ]
    }
   ],
   "source": [
    "# [1] dialfact test\n",
    "FRAME_FILE='/data/scratch/acw722/corefbert/result/resolved_data/dialfact/dialfact_test_with_pronouns.jsonl'\n",
    "# PRED_FILE='/data/scratch/acw722/corefbert/result/inference/nbest_predictions_dialfact_test.json'\n",
    "PRED_FILE='/data/scratch/acw722/corefbert/result/inference/predictions_dialfact_test.json'\n",
    "OUTPUT_FILE='/data/scratch/acw722/corefbert/result/inference/thebest_predictions_dialfact_test.json'\n",
    "\n",
    "select_predictions(FRAME_FILE, PRED_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>> Start selecting predictions.\n",
      ">>>>>>>>>>>>>>>>> len(frames): 1463\n",
      ">>>>>>>>>>>>>>>>> len(preds): 1463\n"
     ]
    }
   ],
   "source": [
    "# [3] augwow dev\n",
    "FRAME_FILE='/data/scratch/acw722/corefbert/result/resolved_data/augwow/augwow_dev_with_pronouns.jsonl'\n",
    "PRED_FILE='/data/scratch/acw722/corefbert/result/inference/nbest_predictions_augwow_dev.json'\n",
    "OUTPUT_FILE='/data/scratch/acw722/corefbert/result/inference/thebest_predictions_augwow_dev.json'\n",
    "\n",
    "select_predictions(FRAME_FILE, PRED_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>> Start selecting predictions.\n",
      ">>>>>>>>>>>>>>>>> len(frames): 168347\n",
      ">>>>>>>>>>>>>>>>> len(preds): 168347\n"
     ]
    }
   ],
   "source": [
    "# [4] augwow train\n",
    "FRAME_FILE='/data/scratch/acw722/corefbert/result/resolved_data/augwow/augwow_train_with_pronouns.jsonl'\n",
    "# PRED_FILE='/data/scratch/acw722/corefbert/result/inference/nbest_predictions_augwow_train.json'\n",
    "PRED_FILE='/data/scratch/acw722/corefbert/result/inference/predictions_augwow_train.json'\n",
    "OUTPUT_FILE='/data/scratch/acw722/corefbert/result/inference/thebest_predictions_augwow_train.json'\n",
    "\n",
    "select_predictions(FRAME_FILE, PRED_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"Game of Thrones\"를 가장 자연스러운 선택으로 예측하는 데 있어서 GPT-2가 BERT보다 더 성공적이었던 이유?\n",
    "\n",
    "    - 학습 목표와 방식의 차이: GPT-2는 연속된 텍스트를 예측하는 언어 생성 모델입니다. 이는 주어진 문맥을 바탕으로 다음에 올 텍스트를 예측하는 방식으로 학습되며, 전체 문장의 흐름과 문맥적 자연스러움에 중점을 둡니다. 반면, BERT는 문장 내 빈칸을 채우는 방식(Masked Language Model, MLM)으로 학습되어, 주어진 문맥 내에서 단어나 구의 적합성을 평가하는 데 초점을 맞춥니다. 이러한 차이로 인해, 전체 문장의 자연스러움을 평가하는 작업에서 GPT-2가 더 유리할 수 있습니다.\n",
    "\n",
    "    - 언어 이해 및 생성 능력: GPT-2는 문장 생성 작업에 특화된 모델로, 주어진 문맥을 기반으로 문장을 이어 나가는 능력이 매우 뛰어납니다. 이는 모델이 전체 문장 구조와 문맥을 더 효과적으로 이해하고, 그에 따라 더 자연스러운 텍스트를 생성할 수 있음을 의미합니다. 따라서, \"Game of Thrones\"와 같이 특정 문맥에서 자연스럽게 등장할 수 있는 구나 문구를 예측하는 데 있어 GPT-2가 더 정확할 수 있습니다.\n",
    "\n",
    "    - 토큰 처리 방식의 차이: BERT는 주로 문장 내의 단일 토큰이나 짧은 구를 대상으로 학습되며, 한 번에 하나의 [MASK] 토큰만 예측합니다. 반면, GPT-2는 전체 문장을 생성하는 과정에서 여러 단어나 구를 연속적으로 예측할 수 있습니다. 이로 인해, \"Game of Thrones\"와 같이 여러 토큰으로 구성된 구나 문구를 더 자연스럽게 처리하고 예측할 수 있습니다.\n",
    "\n",
    "    - 이러한 이유들로 인해, 주어진 문장 내에서 \"Game of Thrones\"를 가장 적합한 선택으로 예측하는 데 GPT-2가 BERT보다 더 성공적일 수 있습니다. GPT-2의 학습 방식과 모델 구조는 전체 문장의 문맥적 자연스러움과 흐름을 더 잘 파악하고 반영할 수 있기 때문입니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "corefenv",
   "language": "python",
   "name": "corefenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
